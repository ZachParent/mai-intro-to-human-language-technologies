{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d17522b",
   "metadata": {},
   "source": [
    "# IHLT Lab 6: Word Sense Disambiguation\n",
    "\n",
    "**Authors:** *Zachary Parent ([zachary.parent](mailto:zachary.parent@estudiantat.upc.edu)), Carlos Jim√©nez ([carlos.humberto.jimenez](mailto:carlos.humberto.jimenez@estudiantat.upc.edu))*\n",
    "\n",
    "### 2024-10-24\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "1. Read all pairs of sentences of the SMTeuroparl files of test set within the evaluation framework of the project.\n",
    "\n",
    "2. Apply WSD algorithms to the words in the sentences.\n",
    "\n",
    "3. Compute their similarities by considering senses and Jaccard coefficient.\n",
    "\n",
    "4. Compare the results with those in session 2 (document) and 3 (morphology) in which words and lemmas were considered.\n",
    "\n",
    "5. Compare the results with gold standard by giving the pearson correlation between them.\n",
    "\n",
    "## Notes\n",
    "\n",
    "* this time we are forced to use TextServer (only session)\n",
    "* spoiler alert: using word sense will not work as well\n",
    "    * best approach is morphology\n",
    "* if we combine approaches from symset, lemmas, tokens into one model, we will probably get better results\n",
    "* could use lemma tokens as a fallback\n",
    "* could combine similarity metrics, such as all the previous methods, and use a model, like KNN or decision tree, to get better results on the project\n",
    "* could use metadata for features, like document length, parts of speech\n",
    "* lesk is quite bad, UKB is slightly better\n",
    "* the wn response is a different format than we are used to, e.g. `10285313-n`\n",
    "    * to get the symset, use `wn.synset_from_pos_and_offset('n',10285313)`\n",
    "* textserver python API is not working, but we could use the browser console to hit the web API\n",
    "* the browser GUI works too\n",
    "* it's a good idea to cache the results from textserver\n",
    "* we will decide what to do with tokens that don't have a sense, we could ignore the ones without a sense. if we keep the ones, we will apply the same pre-processing as in session 2 and 3\n",
    "    * he's being coy, but we should probably not discard the words\n",
    "    * if the API doesn't work, it will be OK to not use UKB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54ace28a",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc74633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.corpus import wordnet_ic\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import pearsonr\n",
    "from nltk.metrics.distance import jaccard_distance\n",
    "from functools import partial\n",
    "import itertools\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from textserver import TextServer\n",
    "import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a4d3b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zachparent/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/zachparent/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     /Users/zachparent/nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet_ic to\n",
      "[nltk_data]     /Users/zachparent/nltk_data...\n",
      "[nltk_data]   Package wordnet_ic is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('sentiwordnet')\n",
    "nltk.download('wordnet_ic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "079d49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ts = ts = TextServer(config.TEXT_SERVER_USERNAME, config.TEXT_SERVER_PASSWORD, 'morpho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9000c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = ['I', 'went', 'to', 'the', 'bank', 'to', 'deposit', 'money', '.']\n",
    "synset = nltk.wsd.lesk(context, 'bank', 'n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9231aef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('savings_bank.n.02',\n",
       " 'a container (usually with a slot in the top) for keeping money at home')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "synset.name(), synset.definition()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa6dcc7",
   "metadata": {},
   "source": [
    "### 1. Read all pairs of sentences of the SMTeuroparl files of test set within the evaluation framework of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "04b09438",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "567647fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert BASE_PATH is not None, \"BASE_PATH is not set\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d693932",
   "metadata": {},
   "source": [
    "## Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c4c69c2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>gs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The leaders have now been given a new chance a...</td>\n",
       "      <td>The leaders benefit aujourd' hui of a new luck...</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Amendment No 7 proposes certain changes in the...</td>\n",
       "      <td>Amendment No 7 is proposing certain changes in...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Let me remind you that our allies include ferv...</td>\n",
       "      <td>I would like to remind you that among our alli...</td>\n",
       "      <td>4.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The vote will take place today at 5.30 p.m.</td>\n",
       "      <td>The vote will take place at 5.30pm</td>\n",
       "      <td>4.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
       "      <td>The fishermen are inactive, tired and disappoi...</td>\n",
       "      <td>5.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  s1  \\\n",
       "0  The leaders have now been given a new chance a...   \n",
       "1  Amendment No 7 proposes certain changes in the...   \n",
       "2  Let me remind you that our allies include ferv...   \n",
       "3        The vote will take place today at 5.30 p.m.   \n",
       "4  The fishermen are inactive, tired and disappoi...   \n",
       "\n",
       "                                                  s2    gs  \n",
       "0  The leaders benefit aujourd' hui of a new luck...  4.50  \n",
       "1  Amendment No 7 is proposing certain changes in...  5.00  \n",
       "2  I would like to remind you that among our alli...  4.25  \n",
       "3                 The vote will take place at 5.30pm  4.50  \n",
       "4  The fishermen are inactive, tired and disappoi...  5.00  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt = pd.read_csv(\n",
    "    f\"{BASE_PATH}/test-gold/STS.input.SMTeuroparl.txt\", sep=\"\\t\", header=None\n",
    ")\n",
    "dt.columns = [\"s1\", \"s2\"]\n",
    "gs = pd.read_csv(\n",
    "    f\"{BASE_PATH}/test-gold/STS.gs.SMTeuroparl.txt\", sep=\"\\t\", header=None\n",
    ")\n",
    "dt[\"gs\"] = gs[0]\n",
    "dt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fab212dc",
   "metadata": {},
   "source": [
    "## Previous results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a20b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization methods\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_one(word):\n",
    "    x, pos = nltk.pos_tag([word])[0]\n",
    "    d = {'NN': 'n', 'NNS': 'n', \n",
    "       'JJ': 'a', 'JJR': 'a', 'JJS': 'a', \n",
    "       'VB': 'v', 'VBD': 'v', 'VBG': 'v', 'VBN': 'v', 'VBP': 'v', 'VBZ': 'v', \n",
    "       'RB': 'r', 'RBR': 'r', 'RBS': 'r'}\n",
    "    if pos in d:\n",
    "        return wnl.lemmatize(word, pos=d[pos])\n",
    "    return x\n",
    "\n",
    "def lemmatize_many(words):\n",
    "    return [lemmatize_one(word) for word in words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49161e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Token pre-processing methods\n",
    "def remove_non_alnum(tokens):\n",
    "    return [token for token in tokens if token.isalnum()]\n",
    "\n",
    "def lower(tokens):\n",
    "    return [token.lower() for token in tokens]\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in nltk.corpus.stopwords.words(\"english\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0269a015",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scoring methods\n",
    "def jaccard_vector(s1, s2):\n",
    "    return pd.concat([s1, s2], axis=1).apply(lambda x: jaccard_distance(set(x.iloc[0]), set(x.iloc[1])), axis=1)\n",
    "\n",
    "def score_jaccard_vector(jaccard_vector):\n",
    "    return pearsonr(gs[0], jaccard_vector)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e243c1a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lemmatize</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>-0.490289</td>\n",
       "      <td>-0.503693</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tokenize  lemmatize\n",
       "score -0.490289  -0.503693"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=pd.DataFrame(index=['score'])\n",
    "\n",
    "s1_tokens = dt['s1'].apply(nltk.word_tokenize).apply(remove_non_alnum).apply(lower)\n",
    "s2_tokens = dt['s2'].apply(nltk.word_tokenize).apply(remove_non_alnum).apply(lower)\n",
    "results['tokenize'] = score_jaccard_vector(jaccard_vector(s1_tokens, s2_tokens))\n",
    "\n",
    "s1_lemmas = dt['s1'].apply(nltk.word_tokenize).apply(remove_non_alnum).apply(lower).apply(remove_stopwords).apply(lemmatize_many)\n",
    "s2_lemmas = dt['s2'].apply(nltk.word_tokenize).apply(remove_non_alnum).apply(lower).apply(remove_stopwords).apply(lemmatize_many)\n",
    "results['lemmatize'] = score_jaccard_vector(jaccard_vector(s1_lemmas, s2_lemmas))\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13177672",
   "metadata": {},
   "source": [
    "### 2. Apply WSD algorithms to the words in the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4c149f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "CACHE_FILE = 'textserver_cache.pkl'\n",
    "\n",
    "def load_cache():\n",
    "    try:\n",
    "        with open(CACHE_FILE, 'rb') as f:\n",
    "            return pickle.load(f)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "cache = load_cache()\n",
    "\n",
    "def dump_cache():\n",
    "    with open(CACHE_FILE, 'wb') as f:\n",
    "        pickle.dump(cache, f)\n",
    "\n",
    "def get_textserver_response(s):\n",
    "    if s in cache:\n",
    "        return cache[s]\n",
    "    try:\n",
    "        response = ts.senses(s)\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response for {s}: {e}\")\n",
    "        raise e\n",
    "    cache[s] = response\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0df09399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lesk_synset_if_exists(tokens):\n",
    "    synsets = [nltk.wsd.lesk(tokens, word) for word in tokens]\n",
    "    synsets_or_tokens = [synset.name() if synset is not None else token for synset, token in zip(synsets, tokens)]\n",
    "    return synsets_or_tokens\n",
    "\n",
    "def get_ukb_synset_if_exists(tokens):\n",
    "    try:\n",
    "        response = get_textserver_response(' '.join(tokens))\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting response for {tokens}: {e}\")\n",
    "        return tokens\n",
    "    synset_accessors = [sense[4] for sense in response[0]]\n",
    "    synsets = [wn.synset_from_pos_and_offset( x.split('-')[1], int( x.split('-')[0])).name() if x != 'N/A' else None for x in synset_accessors]\n",
    "    synsets_or_tokens = [synset if synset is not None else token for synset, token in zip(synsets, tokens)]\n",
    "    return synsets_or_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "f21b7c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_lesk_synset_if_exists(s1_lemmas[0]): ['leader.n.01', 'render.v.04', 'newfangled.s.01', 'gamble.v.01', 'permit.v.01', 'uranium.n.01', 'promise.n.02', 'assume.v.06']\n",
      "Error getting response for leader give new chance let us hope seize: 401 Client Error:  for url: http://frodo.lsi.upc.edu:8080/TextWS/textservlet/ws/processQuery/senses\n",
      "Error getting response for ['leader', 'give', 'new', 'chance', 'let', 'us', 'hope', 'seize']: 401 Client Error:  for url: http://frodo.lsi.upc.edu:8080/TextWS/textservlet/ws/processQuery/senses\n",
      "get_ukb_synset_if_exists(s1_lemmas[0]): ['leader', 'give', 'new', 'chance', 'let', 'us', 'hope', 'seize']\n"
     ]
    }
   ],
   "source": [
    "print(f\"get_lesk_synset_if_exists(s1_lemmas[0]): {get_lesk_synset_if_exists(s1_lemmas[0])}\")\n",
    "print(f\"get_ukb_synset_if_exists(s1_lemmas[0]): {get_ukb_synset_if_exists(s1_lemmas[0])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b790f34",
   "metadata": {},
   "source": [
    "### 3. Compute their similarities by considering senses and Jaccard coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "039008b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tokenize</th>\n",
       "      <th>lemmatize</th>\n",
       "      <th>lesk_synsets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>score</th>\n",
       "      <td>-0.490289</td>\n",
       "      <td>-0.503693</td>\n",
       "      <td>-0.500498</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       tokenize  lemmatize  lesk_synsets\n",
       "score -0.490289  -0.503693     -0.500498"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1_with_lesk_synsets = s1_lemmas.apply(get_lesk_synset_if_exists)\n",
    "s2_with_lesk_synsets = s2_lemmas.apply(get_lesk_synset_if_exists)\n",
    "results['lesk_synsets'] = score_jaccard_vector(jaccard_vector(s1_with_lesk_synsets, s2_with_lesk_synsets))\n",
    "\n",
    "s1_with_ukb_synsets = s1_lemmas.apply(get_ukb_synset_if_exists)\n",
    "s2_with_ukb_synsets = s2_lemmas.apply(get_ukb_synset_if_exists)\n",
    "results['ukb_synsets'] = score_jaccard_vector(jaccard_vector(s1_with_ukb_synsets, s2_with_ukb_synsets))\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a0c40064",
   "metadata": {},
   "outputs": [],
   "source": [
    "dump_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f449f295",
   "metadata": {},
   "source": [
    "### 4. Results comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ab671a",
   "metadata": {},
   "source": [
    "#### Lab2 (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ec78a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de41a2d",
   "metadata": {},
   "source": [
    "#### Lab3 (morphology)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaccea79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e5ca8a26",
   "metadata": {},
   "source": [
    "### gold standard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5f1de50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0df2fd07",
   "metadata": {},
   "source": [
    "# Analysis & Conclusions\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
