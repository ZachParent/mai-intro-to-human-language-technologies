\documentclass{beamer}
\usepackage{listings}
\lstset{
    language=Python,
    basicstyle=\tiny\ttfamily,
    numbers=left,
    numberstyle=\tiny,
    frame=single
}

% Theme and color settings
\usetheme{CambridgeUS}
\usecolortheme{default}

% Package imports
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage{url}

% Title information
\title{Naïve Semantic Text Similarity Model}
\author{Zachary Parent}
\institute{UPC}
\date{\today}

\begin{document}

% Title frame
\begin{frame}
    \titlepage
\end{frame}

% Table of contents
\begin{frame}{Outline}
    \tableofcontents
\end{frame}

% Introduction
\section{Introduction}
\begin{frame}{Introduction}
    \begin{itemize}
        \item Semantic Text Similarity (STS) is crucial for many NLP tasks
        \item Challenge: Which features best capture semantic similarity?
        \item Our approach: Unbiased feature analysis using Random Forests
    \end{itemize}
\end{frame}

% Approach
\section{Methodology}
\begin{frame}{Methodology}
    \begin{itemize}
        \item Approach
        \item Feature extraction
        \item Feature selection
        \item Model training
        \item Model evaluation
    \end{itemize}
\end{frame}

\subsection{Approach}
\begin{frame}{Approach}
    \begin{itemize}
        \item Naïve approach which requires no knowledge of the corpus
        \item Use categorized steps to process sentences in every permutation
        \begin{itemize}
            \item 521 permutations
            \item e.g. sentence\_to\_doc, chunk\_NEs, remove\_stopwords, lemmatize\_tokens, get\_characters, get\_2grams
        \end{itemize}
        \item Apply 4 similarity metrics to each permutation
        \item Used Random Forest's feature importance capabilities
        \item Let the data guide feature selection
    \end{itemize}

\end{frame}

\subsection{Feature Extraction}
\begin{frame}[fragile]{Feature Extraction}
    \begin{lstlisting}[language=Python]
def generate_valid_permutations(
    functions: List[Callable] = all_functions,
) -> List[Tuple[Callable, ...]]:
    valid_permutations = []
    for n in range(1, len(functions) + 1):
        for perm in itertools.permutations(functions, n):
            if _is_valid_permutation(perm):
                valid_permutations.append(perm)
    valid_permutations = (
        [tuple([sentence_to_doc]) + perm for perm in valid_permutations])
    valid_permutations = (
        [new_perm for perm in valid_permutations for new_perm in add_final_step(perm)])
    return valid_permutations

    \end{lstlisting}
\end{frame}

% Results
\section{Results}
\begin{frame}{Top Features}
    \begin{itemize}
        \item Jaccard similarity dominates (7 of top 10)
        \item Common pipeline steps: lemmatization, stopwords, n-grams
        \item Top feature accounts for 20\% importance
    \end{itemize}
    \begin{figure}[h]
        \centering
        \begin{tabular}{|l|c|}
            \hline
            \textbf{Feature Pipeline} & \textbf{Importance} \\
            \hline
            score\_jaccard\_165 & 0.197 \\
            score\_cosine\_257 & 0.089 \\
            score\_cosine\_165 & 0.069 \\
            score\_jaccard\_258 & 0.033 \\
            score\_cosine\_258 & 0.022 \\
            \hline
        \end{tabular}
        \caption{Top 5 Features by Importance}
    \end{figure}
\end{frame}

% Conclusions
\section{Conclusions}
\begin{frame}{Conclusions}
    \begin{itemize}
        \item Simple features can be highly effective
        \item Pipeline complexity isn't always better
        \item Character-level analysis with n-grams shows promise
    \end{itemize}
\end{frame}

\end{document}
